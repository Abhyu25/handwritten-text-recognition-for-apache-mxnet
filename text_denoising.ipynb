{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Denoising\n",
    "\n",
    "Inspired by \"Neural Networks for Text Correction and Completion in Keyboard Decoding\" by Shaona Ghosh and Per Ola Kristensson. https://arxiv.org/pdf/1709.06429.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Append, Pad\n",
    "import gluonnlp as nlp\n",
    "import mxboard\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import HybridBlock\n",
    "from mxnet.gluon.loss import SoftmaxCELoss\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.encoder_decoder import get_transformer_encoder_decoder, Denoiser, encode_char, decode_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('dataset'):\n",
    "    os.makedirs('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "typo_filepath = 'dataset/typo/typo-corpus-r1.txt'\n",
    "text_filepath = 'dataset/typo/alicewonder.txt'\n",
    "mx.test_utils.download('http://luululu.com/tweet/typo-corpus-r1.txt', dirname='dataset/typo')\n",
    "mx.test_utils.download('http://textfiles.com/etext/FICTION/alicewonder.txt', dirname='dataset/typo')\n",
    "# This needs to be made available somewhere\n",
    "text_filepath = 'dataset/typo/all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = mxboard.SummaryWriter(logdir='logs', flush_secs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = ['<UNK>', '<PAD>', '<BOS>', '<EOS>']+list(' ' + string.ascii_letters + string.digits + string.punctuation)\n",
    "ALPHABET_INDEX = {letter: index for index, letter in enumerate(ALPHABET)} # { a: 0, b: 1, etc}\n",
    "FEATURE_LEN = 150 # max-length in characters for one document\n",
    "NUM_WORKERS = 8 # number of workers used in the data loading\n",
    "BATCH_SIZE = 64 # number of documents per batch\n",
    "MAX_LEN_SENTENCE = 150\n",
    "PAD = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "UNK = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the vocabulary for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines = [l.replace('\\n','').replace('`','\"').replace('--',' -- ').strip() for l in open(text_filepath, 'r', encoding='Latin-1').readlines() if l != \"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ' '.join(text_lines)\n",
    "tokenizer = nlp.data.transforms.SpacyTokenizer()\n",
    "tokens = tokenizer(full_text[:999999])\n",
    "counter = nlp.data.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(counter, unknown_token='<UNK>', padding_token='<PAD>',\n",
    "                  bos_token='<BOS>', eos_token='<EOS>', min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_words:\n",
    "    import nltk\n",
    "    nltk.download('perluniprops')\n",
    "    nltk.download('nonbreaking_prefixes')\n",
    "    glove_embed = nlp.embedding.create('glove', source='glove.6B.50d')\n",
    "    \n",
    "    tokenizer = nlp.data.transforms.NLTKMosesTokenizer()\n",
    "    tokens = tokenizer(' '.join(text_lines))\n",
    "    counter = nlp.data.Counter(tokens)\n",
    "    vocab = nlp.Vocab(counter, unknown_token='<UNK>', padding_token='<PAD>',\n",
    "                      bos_token='<BOS>', eos_token='<EOS>', min_freq=1)\n",
    "    vocab.set_embedding(glove_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTextDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, text_filepath, typo_filepath, substitute_costs_filepath='models/substitute_probs.json', \n",
    "                 replace_weight=0, insert_weight=1, delete_weight=1, glue_prob=0.05, substitute_weight=2,\n",
    "                 max_replace=0.3,\n",
    "                 is_train=True, split=0.9,\n",
    "                ):\n",
    "        self.max_replace = max_replace\n",
    "        self.replace_weight = 0 #replace_prob  #Ignore typo dataset\n",
    "        self.substitute_threshold = float(substitute_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.insert_threshold = self.substitute_threshold + float(insert_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.delete_threshold = self.insert_threshold + float(delete_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.glue_prob = glue_prob\n",
    "        self.typo_dict = self._process_typo(typo_filepath)\n",
    "        self.substitute_dict = json.load(open(substitute_costs_filepath,'r'))\n",
    "        self.split = split\n",
    "        self.text = self._process_text(text_filepath, is_train)\n",
    "\n",
    "        \n",
    "    def _process_text(self, filename, is_train):\n",
    "        with open(filename, 'r', encoding='Latin-1') as f:\n",
    "            text = []\n",
    "            for line in f.readlines():\n",
    "                if line != '':\n",
    "                    text.append(line.strip())\n",
    "            \n",
    "            split_index = int(self.split*len(text))\n",
    "            if is_train:\n",
    "                text = text[:split_index]\n",
    "            else:\n",
    "                text = text[split_index:]\n",
    "        return text\n",
    "\n",
    "    def _process_typo(self, filename):\n",
    "        \"\"\"\n",
    "        This function loads the typo dataset and generate the \n",
    "        probability distribution of typos for each valid word\n",
    "        \"\"\"\n",
    "        typo_dict = defaultdict(lambda : defaultdict(float))\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                typo, correct = line.split('\\t')[0:2]\n",
    "                typo_dict[correct][typo] += 1\n",
    "        for _, correct_word in typo_dict.items():\n",
    "            total = 0\n",
    "            for _, count in correct_word.items():\n",
    "                total += count\n",
    "            previous_value = 0.\n",
    "            for wrong_word in correct_word:\n",
    "                correct_word[wrong_word] = correct_word[wrong_word] / total + previous_value\n",
    "                previous_value = correct_word[wrong_word]\n",
    "        return typo_dict\n",
    "    \n",
    "    def _transform_line(self, line):\n",
    "        \"\"\"\n",
    "        replace words that are in the typo dataset with a typo\n",
    "        with a probability `self.replace_proba`\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        \n",
    "        processed_line = self._pre_process_line(line)\n",
    "        \n",
    "        # We get randomly the index of the modifications\n",
    "        num_chars = len(''.join(processed_line))\n",
    "        if num_chars:\n",
    "            index_modifications = np.random.choice(num_chars, random.randint(0, int(self.max_replace*num_chars)), replace=False)\n",
    "            substitute_letters = []\n",
    "            insert_letters = []\n",
    "            delete_letters = []\n",
    "            # We randomly assign these indices to modifications based on precalculated thresholds\n",
    "            for index in index_modifications:\n",
    "                draw = random.random()\n",
    "                if draw < self.substitute_threshold:\n",
    "                    substitute_letters.append(index)\n",
    "                    continue\n",
    "                if draw < self.insert_threshold:\n",
    "                    insert_letters.append(index)\n",
    "                    continue\n",
    "                else:\n",
    "                    delete_letters.append(index)\n",
    "                            \n",
    "        \n",
    "        j = 0\n",
    "        for i, word in enumerate(processed_line):\n",
    "            if word != '' and word not in string.punctuation:\n",
    "                len_word = len(word)\n",
    "                ###########################\n",
    "                #          IGNORED        #\n",
    "                ###########################\n",
    "                #if word.lower() in self.typo_dict: \n",
    "                #    # Replace word with a typo based on probability distribution\n",
    "                #    if random.random() < self.replace_prob:\n",
    "                #        draw = random.random()\n",
    "                #        for typo, value in self.typo_dict[word].items():\n",
    "                #            if draw < value:\n",
    "                #                word = self._match_caps(word, typo)\n",
    "                #                break\n",
    "                    \n",
    "                # Replace letter with substitute based on probability distribution\n",
    "                word_ = []\n",
    "                k = j\n",
    "                for letter in word:\n",
    "                    if k in substitute_letters and letter in self.substitute_dict:\n",
    "                        draw = random.random()\n",
    "                        for replace, prob in self.substitute_dict[letter].items():\n",
    "                            if draw < prob:\n",
    "                                letter = replace\n",
    "                                break\n",
    "                    word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                                \n",
    "                # Insert random letter\n",
    "                k = j\n",
    "                word_ = []\n",
    "                for letter in word:\n",
    "                    if k in insert_letters:\n",
    "                        word_.append(ALPHABET[random.randint(4, len(ALPHABET)-1)])\n",
    "                    word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                \n",
    "                # Delete random letter\n",
    "                k = j\n",
    "                word_ = []\n",
    "                for letter in word:\n",
    "                    if k not in delete_letters:\n",
    "                        word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                    \n",
    "                output.append(word)\n",
    "            else:\n",
    "                output.append(word)\n",
    "            j += len(word)\n",
    "\n",
    "        output_ = [\"\"]*len(output)\n",
    "        j = 0\n",
    "        for i, word in enumerate(output):\n",
    "            output_[j] += word\n",
    "            if random.random() > self.glue_prob:\n",
    "                j += 1\n",
    "        \n",
    "        line = self._post_process_line(output_)\n",
    "        return line.strip()\n",
    "    \n",
    "    def _pre_process_line(self, line):\n",
    "        line = line.replace('\\n','').replace('`',\"'\").replace('--',' -- ')\n",
    "        for char in string.punctuation:\n",
    "            if char in line:\n",
    "                line = line.replace(char, ' '+char+' ')\n",
    "        return line.split(' ')\n",
    "        \n",
    "    def _post_process_line(self, words):\n",
    "        output = ' '.join(words)\n",
    "        for char in string.punctuation:\n",
    "            output = output.replace(' '+char+' ', char)\n",
    "        return output\n",
    "    \n",
    "    def _match_caps(self, original, typo):\n",
    "        if original.isupper():\n",
    "            return typo.upper()\n",
    "        elif original.istitle():\n",
    "            return typo.capitalize()\n",
    "        else:\n",
    "            return typo\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        line = self.text[idx]\n",
    "        line_typo = self._transform_line(line)\n",
    "        return line_typo, line\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(text, src=True):\n",
    "    encoded = np.ones(FEATURE_LEN, dtype='float32') * PAD\n",
    "    text = text[:FEATURE_LEN-2]\n",
    "    i = 0\n",
    "    if not src:\n",
    "        encoded[0] = BOS\n",
    "        i = 1\n",
    "    for letter in text:\n",
    "        if letter in ALPHABET_INDEX:\n",
    "            encoded[i] = ALPHABET_INDEX[letter]\n",
    "        i += 1\n",
    "    encoded[i] = EOS\n",
    "    return encoded, np.array([i+1]).astype('float32')\n",
    "\n",
    "def encode_word(text, src=True):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = vocab[tokens]\n",
    "    indices += [vocab['<EOS>']]\n",
    "    indices = [vocab['<BOS>']]+indices\n",
    "    return indices, np.array([len(indices)]).astype('float32')\n",
    "\n",
    "def transform(data, label):\n",
    "    src, src_valid_length = encode_char(data, src=True)\n",
    "    tgt, tgt_valid_length = encode_char(label, src=False)\n",
    "    return src, src_valid_length, tgt, tgt_valid_length, data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NoisyTextDataset(text_filepath=text_filepath, typo_filepath=typo_filepath, glue_prob=0.2, is_train=True).transform(transform)\n",
    "dataset_test = NoisyTextDataset(text_filepath=text_filepath, typo_filepath=typo_filepath, glue_prob=0.2, is_train=False).transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([27., 19., 18., 73., 24.,  4.,  5., 15.,  9.,  4., 24., 12.,  9.,\n",
       "        23.,  9.,  4., 24., 12., 13., 18., 11., 23., 78.,  4., 23., 19.,\n",
       "         4., 12.,  9., 24., 11.,  4., 17.,  9., 78.,  5., 73., 16., 16.,\n",
       "         4.,  8., 16., 16.,  4., 12.,  9., 22.,  3.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([48.], dtype=float32),\n",
       " array([ 2., 27., 19., 18., 73., 24.,  4., 24.,  5., 15.,  9.,  4., 24.,\n",
       "        12.,  9., 23.,  9.,  4., 24., 12., 13., 18., 11., 23., 78.,  4.,\n",
       "        23., 19.,  4., 12.,  9., 16., 20.,  4., 17.,  9., 78.,  4., 39.,\n",
       "        73., 16., 16.,  4., 24.,  9., 16., 16.,  4., 12.,  9., 22.,  3.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([52.], dtype=float32),\n",
       " \"won't ake these things, so hetg me,a'll dll her\",\n",
       " \"won't take these things, so help me, I'll tell her\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[random.randint(0, len(dataset_train)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data being the IAM Dataset prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('dataset/typo/finetuning.json','r'))\n",
    "data_ = []\n",
    "for label, modified in data:\n",
    "    if label.strip() != modified.strip():\n",
    "        data_.append([label, modified])\n",
    "val_dataset_ft = gluon.data.ArrayDataset(list(list(zip(*data_))[1]), list(list(zip(*data_))[0])).transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24., 12.,  5., 18.,  4.,  5.,  4.,  7., 19., 25., 20., 16.,  9.,\n",
       "         4., 19., 10.,  4., 24., 16.,  9., 20., 12., 19., 18.,  9.,  4.,\n",
       "         7.,  5., 16., 16., 23.,  4., 12.,  9.,  4., 27.,  5., 23.,  3.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([39.], dtype=float32),\n",
       " array([ 2., 24., 12.,  5., 18.,  4.,  5.,  4.,  7., 19., 25., 20., 16.,\n",
       "         9.,  4., 19., 10.,  4., 24.,  9., 16.,  9., 20., 12., 19., 18.,\n",
       "         9.,  4.,  7.,  5., 16., 16., 23.,  4., 12.,  9.,  4., 27.,  5.,\n",
       "        23.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([41.], dtype=float32),\n",
       " 'than a couple of tlephone calls he was',\n",
       " 'than a couple of telephone calls he was')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_ft[random.randint(0, len(val_dataset_ft)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_list(elem):\n",
    "    output = []\n",
    "    for e in elem:\n",
    "        output.append(elem)\n",
    "    return output\n",
    "    \n",
    "batchify = Tuple(Stack(), Stack(), Stack(), Stack(), batchify_list, batchify_list)\n",
    "batchify_word = Tuple(Stack(), Stack(), Pad(), Stack(), batchify_list, batchify_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char(text):\n",
    "    output = []\n",
    "    for val in text:\n",
    "        if val == EOS:\n",
    "            break\n",
    "        elif val == PAD or val == BOS:\n",
    "            continue\n",
    "        output.append(ALPHABET[int(val)])\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "detokenizer = nlp.data.NLTKMosesDetokenizer()\n",
    "def decode_word(indices):\n",
    "    return detokenizer([vocab.idx_to_token[int(i)] for i in indices], return_str=True).replace('<PAD>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, last_batch='rollover', batchify_fn=batchify, num_workers=5)\n",
    "test_data = gluon.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, last_batch='rollover', batchify_fn=batchify, num_workers=5)\n",
    "val_data_ft = gluon.data.DataLoader(val_dataset_ft, batch_size=BATCH_SIZE, shuffle=True, last_batch='keep', batchify_fn=batchify, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCEMaskedLoss(SoftmaxCELoss):\n",
    "    \"\"\"Wrapper of the SoftmaxCELoss that supports valid_length as the input\n",
    "    \"\"\"\n",
    "    def hybrid_forward(self, F, pred, label, valid_length): # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        F\n",
    "        pred : Symbol or NDArray\n",
    "            Shape (batch_size, length, V)\n",
    "        label : Symbol or NDArray\n",
    "            Shape (batch_size, length)\n",
    "        valid_length : Symbol or NDArray\n",
    "            Shape (batch_size, )\n",
    "        Returns\n",
    "        -------\n",
    "        loss : Symbol or NDArray\n",
    "            Shape (batch_size,)\n",
    "        \"\"\"\n",
    "        if self._sparse_label:\n",
    "            sample_weight = F.cast(F.expand_dims(F.ones_like(label), axis=-1), dtype=np.float32)\n",
    "        else:\n",
    "            sample_weight = F.ones_like(label)\n",
    "        sample_weight = F.SequenceMask(sample_weight,\n",
    "                                       sequence_length=valid_length,\n",
    "                                       use_sequence_length=True,\n",
    "                                       axis=1)\n",
    "        return super(SoftmaxCEMaskedLoss, self).hybrid_forward(F, pred, label, sample_weight)\n",
    "\n",
    "# pylint: disable=unused-argument\n",
    "class _SmoothingWithDim(mx.operator.CustomOp):\n",
    "    def __init__(self, epsilon=0.1, axis=-1):\n",
    "        super(_SmoothingWithDim, self).__init__(True)\n",
    "        self._epsilon = epsilon\n",
    "        self._axis = axis\n",
    "\n",
    "    def forward(self, is_train, req, in_data, out_data, aux):\n",
    "        inputs = in_data[0]\n",
    "        outputs = ((1 - self._epsilon) * inputs) + (self._epsilon / float(inputs.shape[self._axis]))\n",
    "        self.assign(out_data[0], req[0], outputs)\n",
    "\n",
    "    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n",
    "        self.assign(in_grad[0], req[0], (1 - self._epsilon) * out_grad[0])\n",
    "\n",
    "\n",
    "@mx.operator.register('_smoothing_with_dim')\n",
    "class _SmoothingWithDimProp(mx.operator.CustomOpProp):\n",
    "    def __init__(self, epsilon=0.1, axis=-1):\n",
    "        super(_SmoothingWithDimProp, self).__init__(True)\n",
    "        self._epsilon = float(epsilon)\n",
    "        self._axis = int(axis)\n",
    "\n",
    "    def list_arguments(self):\n",
    "        return ['data']\n",
    "\n",
    "    def list_outputs(self):\n",
    "        return ['output']\n",
    "\n",
    "    def infer_shape(self, in_shape):\n",
    "        data_shape = in_shape[0]\n",
    "        output_shape = data_shape\n",
    "        return (data_shape,), (output_shape,), ()\n",
    "\n",
    "    def declare_backward_dependency(self, out_grad, in_data, out_data):\n",
    "        return out_grad\n",
    "\n",
    "    def create_operator(self, ctx, in_shapes, in_dtypes):\n",
    "        #  create and return the CustomOp class.\n",
    "        return _SmoothingWithDim(self._epsilon, self._axis)\n",
    "# pylint: enable=unused-argument\n",
    "\n",
    "\n",
    "class LabelSmoothing(HybridBlock):\n",
    "    \"\"\"Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : int, default -1\n",
    "        The axis to smooth.\n",
    "    epsilon : float, default 0.1\n",
    "        The epsilon parameter in label smoothing\n",
    "    sparse_label : bool, default True\n",
    "        Whether input is an integer array instead of one hot array.\n",
    "    units : int or None\n",
    "        Vocabulary size. If units is not given, it will be inferred from the input.\n",
    "    prefix : str, default 'rnn_'\n",
    "        Prefix for name of `Block`s\n",
    "        (and name of weight if params is `None`).\n",
    "    params : Parameter or None\n",
    "        Container for weight sharing between cells.\n",
    "        Created if `None`.\n",
    "    \"\"\"\n",
    "    def __init__(self, axis=-1, epsilon=0.1, units=None,\n",
    "                 sparse_label=True, prefix=None, params=None):\n",
    "        super(LabelSmoothing, self).__init__(prefix=prefix, params=params)\n",
    "        self._axis = axis\n",
    "        self._epsilon = epsilon\n",
    "        self._sparse_label = sparse_label\n",
    "        self._units = units\n",
    "\n",
    "    def hybrid_forward(self, F, inputs, units=None): # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        F\n",
    "        inputs : Symbol or NDArray\n",
    "            Shape (batch_size, length) or (batch_size, length, V)\n",
    "        units : int or None\n",
    "        Returns\n",
    "        -------\n",
    "        smoothed_label : Symbol or NDArray\n",
    "            Shape (batch_size, length, V)\n",
    "        \"\"\"\n",
    "        if self._sparse_label:\n",
    "            assert units is not None or self._units is not None, \\\n",
    "                'units needs to be given in function call or ' \\\n",
    "                'instance initialization when sparse_label is False'\n",
    "            if units is None:\n",
    "                units = self._units\n",
    "            inputs = F.one_hot(inputs, depth=units)\n",
    "        if units is None and self._units is None:\n",
    "            return F.Custom(inputs, epsilon=self._epsilon, axis=self._axis,\n",
    "                            op_type='_smoothing_with_dim')\n",
    "        else:\n",
    "            if units is None:\n",
    "                units = self._units\n",
    "            return ((1 - self._epsilon) * inputs) + (self._epsilon / units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = Denoiser(alphabet_size=len(ALPHABET), max_src_length=FEATURE_LEN, max_tgt_length=FEATURE_LEN, num_heads=16, embed_size=256, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser.load_parameters('model_checkpoint/denoiser_highhead.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser.initialize(mx.init.Xavier(), ctx)\n",
    "#denoiser.tgt_embedding[0].params.reset_ctx(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(ALPHABET)\n",
    "#output_dim = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = LabelSmoothing(epsilon=0.01, units=output_dim)\n",
    "loss_function_test = SoftmaxCEMaskedLoss(sparse_label=True)\n",
    "loss_function = SoftmaxCEMaskedLoss(sparse_label=False)\n",
    "trainer = gluon.Trainer(denoiser.collect_params(), 'adam', {'learning_rate':0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(denoiser, iterator):\n",
    "    loss = 0\n",
    "    for i, (src, src_valid_length, tgt, tgt_valid_length, typo, label) in enumerate(iterator):\n",
    "        src = src.as_in_context(ctx)\n",
    "        tgt = tgt.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx).squeeze()\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx).squeeze()\n",
    "        output = denoiser(src, tgt[:,:-1], src_valid_length, tgt_valid_length-1)\n",
    "        ls = loss_function_test(output, tgt[:,1:], tgt_valid_length).mean()\n",
    "        loss += ls.asscalar()\n",
    "    print(\"[Test Typo     ] {}\".format(decode_char(src[0].asnumpy())))\n",
    "    print(\"[Test Predicted] {}\".format(decode_char(output[0].asnumpy().argmax(axis=1))))\n",
    "    print(\"[Test Correct  ] {}\".format(decode_char(tgt[0].asnumpy())))\n",
    "    return loss / (i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning text being the IAM Dataset train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_ft = NoisyTextDataset(text_filepath='dataset/typo/text_train.txt', typo_filepath=typo_filepath, is_train=True, split=1.0).transform(transform)\n",
    "train_data_ft = gluon.data.DataLoader(dataset_train_ft, batch_size=BATCH_SIZE, shuffle=True, last_batch='rollover', batchify_fn=batchify, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Typo     ] rwes. Iom JIan Bawley. Does that mean anything\n",
      "[Test Predicted] ras. I m J n Bawley. Does that mean anything\n",
      "[Test Correct  ] was. I'm Ian Bawley. Does that mean anything\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10538775101304054"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(denoiser, val_data_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(denoiser, epoch, train_iterator, test_iterator):\n",
    "    loss = 0.\n",
    "    for i, (src, src_valid_length, tgt, tgt_valid_length, typo, label) in enumerate(train_iterator):\n",
    "        src = src.as_in_context(ctx)\n",
    "        tgt = tgt.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx).squeeze()\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx).squeeze()\n",
    "        with autograd.record():\n",
    "            output = denoiser(src, tgt[:,:-1], src_valid_length, tgt_valid_length-1)\n",
    "            smoothed_label = label_smoothing(tgt[:,1:])\n",
    "            ls = loss_function(output, smoothed_label, tgt_valid_length).mean()\n",
    "        ls.backward()\n",
    "        trainer.step(src.shape[0])\n",
    "        loss += ls.asscalar()\n",
    "        \n",
    "        if i % 300 == 0:\n",
    "            val_loss = evaluate(denoiser, test_iterator)\n",
    "            sw.add_scalar(tag='Val_Loss_it', value={key:val_loss}, global_step=i+e*len(train_iterator))\n",
    "            sw.add_scalar(tag='Train_Loss_it', value={key:loss/(i+1)}, global_step=i+e*len(train_iterator))\n",
    "            print(\"[Iteration {}   ] {}\".format(i, loss / (i+1)))\n",
    "            print(\"[Train Typo     ] {}\".format(decode_char(src[0].asnumpy())))\n",
    "            print(\"[Train Predicted] {}\".format(decode_char(output[0].asnumpy().argmax(axis=1))))\n",
    "            print(\"[Train Correct  ] {}\".format(decode_char(tgt[0].asnumpy())))\n",
    "            print()\n",
    "            sw.flush()\n",
    "\n",
    "    test_loss = evaluate(denoiser, test_iterator)\n",
    "    print(\"Epoch [{}], Train Loss {:.4f}, Test Loss {:.4f}\".format(e, loss/(i+1), test_loss))\n",
    "    sw.add_scalar(tag='Train_Loss', value={key:loss/(i+1)}, global_step=e)\n",
    "    sw.add_scalar(tag='Test_Loss', value={key:test_loss}, global_step=e)\n",
    "    print()\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "key = 'big_dataset_high_head_0.0001'\n",
    "best_test_loss = 10e20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    test_loss = train_epoch(denoiser, e, train_data, val_data_ft)\n",
    "    if test_loss < best_test_loss:\n",
    "        denoiser.save_parameters('model_checkpoint/denoiser_highhead_2.params')\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "key = 'fine_tuning'\n",
    "for e in range(epochs):\n",
    "    test_loss = train_epoch(denoiser, e, train_data_ft, val_data_ft)\n",
    "    if test_loss < best_test_loss:\n",
    "        denoiser.save_parameters('model_checkpoint/denoiser_ft.params')\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideas:\n",
    "    - per word loss\n",
    "    - tokenize better the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = nlp.model.BeamSearchScorer(alpha=0, K=5, from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = EOS\n",
    "beam_sampler = nlp.model.BeamSearchSampler(beam_size=30,\n",
    "                                           decoder=denoiser.decode_logprob,\n",
    "                                           eos_id=eos_id,\n",
    "                                           scorer=scorer,\n",
    "                                           max_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(data, scores, step):\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(sampler, inputs, begin_states):\n",
    "    samples, scores, valid_lengths = sampler(inputs, begin_states)\n",
    "    samples = samples[0].asnumpy()\n",
    "    scores = scores[0].asnumpy()\n",
    "    valid_lengths = valid_lengths[0].asnumpy()\n",
    "    print('Generation Result:')\n",
    "    for sample in samples:\n",
    "        print(decode_char(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"of fact I'd ashed him last night to depurise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Result:\n",
      "of fact I'd asked him last night to deparite\n",
      "of fact I'd asked him last night to deparise\n",
      "of fact I'd asked him last night to deperite\n",
      "of fact I'd asked him last night to deprivise\n",
      "of fact I'd asked him last night to deprise\n",
      "of fact I'd asked him last night to deposite\n",
      "of fact I'd asked him last night to depurite\n",
      "of fact I'd asked him last night to depurise\n",
      "of fact I'd asked him last night to depirite\n",
      "of fact I'd asked him last night to be purise\n",
      "of fact I'd asked him last night to deprince\n",
      "of fact I'd asked him last night to deparing\n",
      "of fact I'd asked him last night to deprite\n",
      "of fact I'd asked him last night to depinise\n",
      "of fact I'd asked him last night to dephrise\n",
      "of fact I'd asked him last night to deperise\n",
      "of fact I'd asked him last night to depinite\n",
      "of fact I'd asked him last night to deprive\n",
      "of fact I'd asked him last night to depenise\n",
      "of fact I'd asked him last night to dephrite\n",
      "of fact I'd asked him last night to depenite\n",
      "of fact I'd asked him last night to deposing\n",
      "of fact I'd asked him last night to deposible\n",
      "of fact I'd ashed him last night to deparite\n",
      "of fact I'd washed him last night to deparite\n",
      "of fact I'd asked him last night to deposive\n",
      "of fact I'd washed him last night to deprivise\n",
      "of fact I'd asked him last night to depenicle\n",
      "of fact I'd washed him last night to deparise\n",
      "of fact I'd asked him last night to deposiblise\n"
     ]
    }
   ],
   "source": [
    "src_seq, src_valid_length = encode_char(sentence)\n",
    "src_seq = mx.nd.array([src_seq], ctx=ctx)\n",
    "src_valid_length = mx.nd.array(src_valid_length, ctx=ctx)\n",
    "encoder_outputs, _ = denoiser.encode(src_seq, valid_length=src_valid_length)\n",
    "states = denoiser.decoder.init_state_from_encoder(encoder_outputs, \n",
    "                                                  encoder_valid_length=src_valid_length)\n",
    "inputs = mx.nd.full(shape=(1,), ctx=src_seq.context, dtype=np.float32, val=BOS)\n",
    "generate_sequences(beam_sampler, inputs, states,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
