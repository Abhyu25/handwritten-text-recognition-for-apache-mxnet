{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Denoising\n",
    "\n",
    "Inspired by \"Neural Networks for Text Correction and Completion in Keyboard Decoding\" by Shaona Ghosh and Per Ola Kristensson. https://arxiv.org/pdf/1709.06429.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Append, Pad\n",
    "import gluonnlp as nlp\n",
    "import hnswlib # https://github.com/nmslib/hnswlib\n",
    "import mxboard\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocr.utils.encoder_decoder import get_transformer_encoder_decoder, Denoiser, encode_char, decode_char, LabelSmoothing, SoftmaxCEMaskedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See get_mode.py\n",
    "text_filepath = 'dataset/typo/all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = ['<UNK>', '<PAD>', '<BOS>', '<EOS>']+list(' ' + string.ascii_letters + string.digits + string.punctuation)\n",
    "ALPHABET_INDEX = {letter: index for index, letter in enumerate(ALPHABET)} # { a: 0, b: 1, etc}\n",
    "FEATURE_LEN = 150 # max-length in characters for one document\n",
    "NUM_WORKERS = 8 # number of workers used in the data loading\n",
    "BATCH_SIZE = 64 # number of documents per batch\n",
    "MAX_LEN_SENTENCE = 150\n",
    "PAD = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "UNK = 0\n",
    "max_len_vocab = 500000\n",
    "\n",
    "moses_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "moses_tokenizer = nlp.data.SacreMosesTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_index():\n",
    "    model, vocab = nlp.model.big_rnn_lm_2048_512(dataset_name='gbw', pretrained=True, ctx=mx.cpu())\n",
    "\n",
    "    step = 1024\n",
    "    dim = 512\n",
    "    num_elements = max_len_vocab+step\n",
    "    data = np.zeros((num_elements, dim), dtype='float32')\n",
    "    data_labels = np.arange(max_len_vocab)\n",
    "    for i in tqdm(range(1, max_len_vocab, step)):\n",
    "        data[i:i+step,:] = model.embedding(mx.nd.arange(i,i+step)).asnumpy()\n",
    "    # Declaring index\n",
    "    p = hnswlib.Index(space = 'cosine', dim = dim) # possible options are l2, cosine or ip\n",
    "\n",
    "    # Initing index - the maximum number of elements should be known beforehand\n",
    "    p.init_index(max_elements = max_len_vocab, ef_construction = 200, M = 16)\n",
    "\n",
    "    # Element insertion (can be called several times):\n",
    "    p.add_items(data[:max_len_vocab], data_labels)\n",
    "    # Controlling the recall by setting ef:\n",
    "    p.set_ef(50) # ef should always be > k\n",
    "    return p, data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTextDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 text_filepath=None, \n",
    "                 substitute_costs_filepath='models/substitute_probs.json', \n",
    "                 insert_weight=1, \n",
    "                 delete_weight=1, \n",
    "                 glue_prob=0.05, \n",
    "                 substitute_weight=2,\n",
    "                 max_replace=0.3,\n",
    "                 is_train=True, \n",
    "                 split=0.9, \n",
    "                 data_type='corpus', \n",
    "                 gbw_corpus=None,\n",
    "                 knn_index=knn_index,\n",
    "                 knn_data=knn_data,\n",
    "                 knn_vocab=knn_vocab,\n",
    "                 proba_synonym=0.1\n",
    "                ):\n",
    "        self.max_replace = max_replace\n",
    "        self.replace_weight = 0 #replace_prob  #Ignore typo dataset\n",
    "        self.substitute_threshold = float(substitute_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.insert_threshold = self.substitute_threshold + float(insert_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.delete_threshold = self.insert_threshold + float(delete_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.glue_prob = glue_prob\n",
    "        self.substitute_dict = json.load(open(substitute_costs_filepath,'r'))\n",
    "        self.split = split\n",
    "        self.data_type = data_type\n",
    "        if self.data_type == 'corpus':\n",
    "            self.text = self._process_text(text_filepath, is_train)\n",
    "        elif self.data_type == 'GBW':\n",
    "            self.gbw_corpus = gbw_corpus\n",
    "        self.knn_index = knn_index\n",
    "        self.knn_data = knn_data\n",
    "        self.knn_vocab = knn_vocab\n",
    "        self.proba_synonym = proba_synonym\n",
    "    \n",
    "    def _process_text(self, filename, is_train):\n",
    "        with open(filename, 'r', encoding='Latin-1') as f:\n",
    "            text = []\n",
    "            for line in f.readlines():\n",
    "                if line != '':\n",
    "                    text.append(line.strip())\n",
    "            \n",
    "            split_index = int(self.split*len(text))\n",
    "            if is_train:\n",
    "                text = text[:split_index]\n",
    "            else:\n",
    "                text = text[split_index:]\n",
    "        return text\n",
    "    \n",
    "    def _replace_synonym(self, line):\n",
    "        processed_line = self._pre_process_line(line)\n",
    "        words = []\n",
    "        num_words = 100\n",
    "        for i, word in enumerate(processed_line):\n",
    "            draw = random.random()\n",
    "            if word in self.knn_vocab and self.knn_vocab[word] < max_len_vocab and draw < self.proba_synonym and word not in string.punctuation :\n",
    "                index_list = self.knn_index.knn_query(self.knn_data[self.knn_vocab[word]], k=num_words)[0][0]\n",
    "                word = self.knn_vocab.idx_to_token[index_list[random.randint(0,num_words-1)]]\n",
    "            words.append(word)\n",
    "        return self._post_process_line(words)\n",
    "    \n",
    "    def _transform_line(self, line):\n",
    "        \"\"\"\n",
    "        replace words that are in the typo dataset with a typo\n",
    "        with a probability `self.replace_proba`\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        \n",
    "        processed_line = self._pre_process_line(line)\n",
    "        \n",
    "        # We get randomly the index of the modifications\n",
    "        num_chars = len(''.join(processed_line))\n",
    "        if num_chars:\n",
    "            index_modifications = np.random.choice(num_chars, random.randint(0, int(self.max_replace*num_chars)), replace=False)\n",
    "            substitute_letters = []\n",
    "            insert_letters = []\n",
    "            delete_letters = []\n",
    "            # We randomly assign these indices to modifications based on precalculated thresholds\n",
    "            for index in index_modifications:\n",
    "                draw = random.random()\n",
    "                if draw < self.substitute_threshold:\n",
    "                    substitute_letters.append(index)\n",
    "                    continue\n",
    "                if draw < self.insert_threshold:\n",
    "                    insert_letters.append(index)\n",
    "                    continue\n",
    "                else:\n",
    "                    delete_letters.append(index)\n",
    "                            \n",
    "        \n",
    "        j = 0\n",
    "        for i, word in enumerate(processed_line):\n",
    "            \n",
    "            if word != '' and word not in string.punctuation:\n",
    "                \n",
    "                len_word = len(word)\n",
    "                word_ = []\n",
    "                k = j\n",
    "                for letter in word:\n",
    "                    if k in substitute_letters and letter in self.substitute_dict:\n",
    "                        draw = random.random()\n",
    "                        for replace, prob in self.substitute_dict[letter].items():\n",
    "                            if draw < prob:\n",
    "                                letter = replace\n",
    "                                break\n",
    "                    word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                                \n",
    "                # Insert random letter\n",
    "                k = j\n",
    "                word_ = []\n",
    "                for letter in word:\n",
    "                    if k in insert_letters:\n",
    "                        word_.append(ALPHABET[random.randint(4, len(ALPHABET)-1)])\n",
    "                    word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                \n",
    "                # Delete random letter\n",
    "                k = j\n",
    "                word_ = []\n",
    "                for letter in word:\n",
    "                    if k not in delete_letters:\n",
    "                        word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                    \n",
    "                output.append(word)\n",
    "            else:\n",
    "                output.append(word)\n",
    "            j += len(word)\n",
    "\n",
    "        output_ = [\"\"]*len(output)\n",
    "        j = 0\n",
    "        for i, word in enumerate(output):\n",
    "            output_[j] += word\n",
    "            if random.random() > self.glue_prob:\n",
    "                j += 1\n",
    "        \n",
    "        line = self._post_process_line(output_)\n",
    "        return line.strip()\n",
    "    \n",
    "    def _pre_process_line(self, line):\n",
    "        line = line.replace('\\n','').replace('`',\"'\").replace('--',' -- ')\n",
    "        return moses_tokenizer(line)\n",
    "        \n",
    "    def _post_process_line(self, words):\n",
    "        output = ' '.join(moses_detokenizer(words))\n",
    "        return output\n",
    "    \n",
    "    def _match_caps(self, original, typo):\n",
    "        if original.isupper():\n",
    "            return typo.upper()\n",
    "        elif original.istitle():\n",
    "            return typo.capitalize()\n",
    "        else:\n",
    "            return typo\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_type == 'GBW':\n",
    "            tokens = moses_detokenizer(self.gbw_corpus[idx][:-1])\n",
    "            if len(tokens) > 6:\n",
    "                start = random.randint(0, len(tokens)-3)\n",
    "                end = random.randint(start, len(tokens))\n",
    "                tokens = tokens[start:end]\n",
    "            line = ' '.join(tokens)\n",
    "        else:\n",
    "            line = self.text[idx]\n",
    "        line = self._replace_synonym(line)\n",
    "        line_typo = self._transform_line(line)\n",
    "        return line_typo, line\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data_type == 'GBW':\n",
    "            return len(self.gbw_corpus)\n",
    "        else:\n",
    "            return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(text, src=True):\n",
    "    encoded = np.ones(FEATURE_LEN, dtype='float32') * PAD\n",
    "    text = text[:FEATURE_LEN-2]\n",
    "    i = 0\n",
    "    if not src:\n",
    "        encoded[0] = BOS\n",
    "        i = 1\n",
    "    for letter in text:\n",
    "        if letter in ALPHABET_INDEX:\n",
    "            encoded[i] = ALPHABET_INDEX[letter]\n",
    "        i += 1\n",
    "    encoded[i] = EOS\n",
    "    return encoded, np.array([i+1]).astype('float32')\n",
    "\n",
    "def encode_word(text, src=True):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = vocab[tokens]\n",
    "    indices += [vocab['<EOS>']]\n",
    "    indices = [vocab['<BOS>']]+indices\n",
    "    return indices, np.array([len(indices)]).astype('float32')\n",
    "\n",
    "def transform(data, label):\n",
    "    src, src_valid_length = encode_char(data, src=True)\n",
    "    tgt, tgt_valid_length = encode_char(label, src=False)\n",
    "    return src, src_valid_length, tgt, tgt_valid_length, data, label\n",
    "\n",
    "def decode_char(text):\n",
    "    output = []\n",
    "    for val in text:\n",
    "        if val == EOS:\n",
    "            break\n",
    "        elif val == PAD or val == BOS:\n",
    "            continue\n",
    "        output.append(ALPHABET[int(val)])\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "def decode_word(indices):\n",
    "    return detokenizer([vocab.idx_to_token[int(i)] for i in indices], return_str=True).replace('<PAD>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/489 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/489 [00:00<05:01,  1.62it/s]\u001b[A\n",
      " 14%|█▍        | 70/489 [00:00<00:04, 97.53it/s]\u001b[A\n",
      " 28%|██▊       | 139/489 [00:00<00:02, 169.22it/s]\u001b[A\n",
      " 43%|████▎     | 208/489 [00:00<00:01, 225.60it/s]\u001b[A\n",
      " 56%|█████▋    | 276/489 [00:01<00:00, 270.03it/s]\u001b[A\n",
      " 70%|███████   | 343/489 [00:01<00:00, 305.41it/s]\u001b[A\n",
      " 84%|████████▍ | 411/489 [00:01<00:00, 336.10it/s]\u001b[A\n",
      " 98%|█████████▊| 480/489 [00:01<00:00, 362.55it/s]\u001b[A\n",
      "100%|██████████| 489/489 [00:01<00:00, 365.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 20s, sys: 0 ns, total: 45min 20s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We get a knn index to substitute words\n",
    "knn_index, knn_data, knn_vocab  = get_knn_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our synonym replacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'display'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"test\"\n",
    "num_words = 200\n",
    "index_list = knn_index.knn_query(knn_data[knn_vocab[word]], k=1000)[0][0]\n",
    "knn_vocab.idx_to_token[index_list[random.randint(0,num_words-1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NoisyTextDataset(text_filepath=text_filepath, glue_prob=0.2, is_train=True).transform(transform)\n",
    "dataset_test = NoisyTextDataset(text_filepath=text_filepath, glue_prob=0.2, is_train=False).transform(transform)\n",
    "\n",
    "# Finetuning on the text from the IAM dataset\n",
    "dataset_train_ft = NoisyTextDataset(text_filepath='dataset/typo/text_train.txt', is_train=True, split=1.0, knn_index=knn_index).transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([18.,  9., 90., 26., 20.,  9., 22., 24., 12.,  9., 16.,  9., 76.,\n",
       "        23., 23., 78.,  4., 43., 22.,  4., 20., 13., 16., 18.,  9.,  9.,\n",
       "        30.,  9.,  8.,  4., 19., 85., 18.,  9., 78.,  4.,  5., 18.,  8.,\n",
       "        50., 84.,  5., 24.,  4., 11.,  5., 18., 45., 11.,  4., 22., 19.,\n",
       "        25., 18.,  8.,  9.,  8., 78.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([59.], dtype=float32),\n",
       " array([ 2., 18.,  9., 26.,  9., 22., 24., 12.,  9., 16.,  9., 23., 23.,\n",
       "        78.,  4., 43., 22.,  4., 20., 13., 16.,  9.,  4., 27., 12.,  9.,\n",
       "         9., 30.,  9.,  8.,  4., 19., 18.,  9., 78.,  4.,  5., 18.,  8.,\n",
       "         4., 50., 12.,  5., 24.,  4., 11., 19., 18., 11.,  4., 23., 19.,\n",
       "        25., 18.,  8.,  9.,  8., 78.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([59.], dtype=float32),\n",
       " 'ne\\\\vperthele*ss, Mr pilneezed o=ne, andT<at ganOg rounded,',\n",
       " 'nevertheless, Mr pile wheezed one, and That gong sounded,')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[random.randint(0, len(dataset_train)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan are now trying to release all our books 0.4 moment -In progresses\n",
      "We are now trying to release all our books one month in first-place\n",
      "We are nonetheless trying to release all our books one month in decline\n",
      "We are now trying to release all our books one month in advance\n",
      "We had now trying to release all our books one six-months in advance\n",
      "We are now trying to launch all our books one month via advance\n",
      "We are now trying to release Slowly our books one nights in advance\n",
      "We are now trying to capture all Iranʼs books one month in advance\n",
      "We are theoretically trying to release all our books one month in advance\n",
      "We are now Attempting to release everybody Washingtonʼs books nobody month in advance\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset_train[42][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data being the IAM Dataset prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('dataset/typo/validating.json','r'))\n",
    "data_ = []\n",
    "for label, modified in data:\n",
    "    if label.strip() != modified.strip():\n",
    "        data_.append([label, modified])\n",
    "val_dataset_ft = gluon.data.ArrayDataset(list(list(zip(*data_))[1]), list(list(zip(*data_))[0])).transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9., 11.,  5., 22.,  8.,  4., 24., 12., 13., 23.,  4.,  5., 23.,\n",
       "         4.,  5.,  4., 23.,  9., 10., 24., 25., 18., 13., 18., 11.,  4.,\n",
       "        79.,  4., 25., 20.,  4., 11., 22., 19., 23., 23.,  4.,  5., 18.,\n",
       "         8.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([41.], dtype=float32),\n",
       " array([ 2., 48.,  9., 11.,  5., 22.,  8.,  4., 24., 12., 13., 23.,  4.,\n",
       "         5., 23.,  4.,  5.,  4., 23., 19., 10., 24.,  9., 18., 13., 18.,\n",
       "        11., 79., 25., 20.,  4., 20., 22., 19.,  7.,  9., 23., 23., 78.,\n",
       "         4.,  5., 18.,  8.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([44.], dtype=float32),\n",
       " 'egard this as a seftuning - up gross and',\n",
       " 'Regard this as a softening-up process, and')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_ft[random.randint(0, len(val_dataset_ft)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GBW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbw_stream = nlp.data.GBWStream(segment='train', skip_empty=True, bos=None, eos='<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, corpus in enumerate(gbw_stream):\n",
    "    dataset_gbw = NoisyTextDataset(gbw_corpus=corpus, data_type='GBW').transform(transform)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12.,  9., 16., 75.,  8.,  4., 13., 18.,  4., 24., 67., 12.,  9.,\n",
       "         4., 12., 19., 23., 20., 13., 24.,  5., 16.,  4., 22., 27., 13.,\n",
       "        11.,  4., 29.,  5., 25.,  9., 79., 12., 13., 51., 11., 12.,  4.,\n",
       "        66., 31., 16.,  6.,  5., 18., 14., 29.,  4., 14.,  5., 13., 16.,\n",
       "        78.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([54.], dtype=float32),\n",
       " array([ 2., 12.,  9., 16.,  8.,  4., 13., 18.,  4., 24., 12.,  9.,  4.,\n",
       "        12., 19., 23., 20., 13., 24.,  5., 16.,  4., 27., 13., 18., 11.,\n",
       "         4., 11.,  5., 17.,  9., 79., 12., 13., 11., 12.,  4., 31., 16.,\n",
       "         6.,  5., 18., 29.,  4., 14.,  5., 13., 16., 78.,  3.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([50.], dtype=float32),\n",
       " 'hel)d in t!he hospital rwig yaue-hiUgh 9Albanjy jail,',\n",
       " 'held in the hospital wing game-high Albany jail,')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gbw[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_list(elem):\n",
    "    output = []\n",
    "    for e in elem:\n",
    "        output.append(elem)\n",
    "    return output\n",
    "    \n",
    "batchify = Tuple(Stack(), Stack(), Stack(), Stack(), batchify_list, batchify_list)\n",
    "batchify_word = Tuple(Stack(), Stack(), Pad(), Stack(), batchify_list, batchify_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, last_batch='rollover', batchify_fn=batchify, num_workers=5)\n",
    "test_data = gluon.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True, last_batch='keep', batchify_fn=batchify, num_workers=5)\n",
    "val_data_ft = gluon.data.DataLoader(val_dataset_ft, batch_size=BATCH_SIZE, shuffle=True, last_batch='keep', batchify_fn=batchify, num_workers=0)\n",
    "train_data_ft = gluon.data.DataLoader(dataset_train_ft, batch_size=BATCH_SIZE, shuffle=True, last_batch='rollover', batchify_fn=batchify, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to help train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, iterator):\n",
    "    loss = 0\n",
    "    for i, (src, src_valid_length, tgt, tgt_valid_length, typo, label) in enumerate(iterator):\n",
    "        src = src.as_in_context(ctx)\n",
    "        tgt = tgt.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx).squeeze()\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx).squeeze()\n",
    "        output = net(src, tgt[:,:-1], src_valid_length, tgt_valid_length-1)\n",
    "        ls = loss_function_test(output, tgt[:,1:], tgt_valid_length).mean()\n",
    "        loss += ls.asscalar()\n",
    "    print(\"[Test Typo     ] {}\".format(decode_char(src[0].asnumpy())))\n",
    "    print(\"[Test Predicted] {}\".format(get_sentence(net, decode_char(src[0].asnumpy()))))\n",
    "    print(\"[Test Correct  ] {}\".format(decode_char(tgt[0].asnumpy())))\n",
    "    return loss / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(net, epoch, train_iterator, test_iterator, trainer):\n",
    "    loss = 0.\n",
    "    for i, (src, src_valid_length, tgt, tgt_valid_length, typo, label) in enumerate(train_iterator):\n",
    "        src = src.as_in_context(ctx)\n",
    "        tgt = tgt.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx).squeeze()\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx).squeeze()\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = net(src, tgt[:,:-1], src_valid_length, tgt_valid_length-1)\n",
    "            smoothed_label = label_smoothing(tgt[:,1:])\n",
    "            ls = loss_function(output, smoothed_label, tgt_valid_length).mean()\n",
    "        \n",
    "        ls.backward()\n",
    "        trainer.step(src.shape[0])\n",
    "        loss += ls.asscalar()\n",
    "        \n",
    "        if i % send_every_n == 0:\n",
    "            val_loss = evaluate(net, test_iterator)\n",
    "            sw.add_scalar(tag='Val_Loss_it', value={key:val_loss}, global_step=i+e*len(train_iterator))\n",
    "            sw.add_scalar(tag='Train_Loss_it', value={key:loss/(i+1)}, global_step=i+e*len(train_iterator))\n",
    "            print(\"[Iteration {} Train] {}\".format(i, loss / (i+1)))\n",
    "            print(\"[Iteration {} Test ] {}\".format(i, val_loss))\n",
    "            print(\"[Train Typo        ] {}\".format(decode_char(src[0].asnumpy())))\n",
    "            print(\"[Train Predicted   ] {}\".format(decode_char(output[0].asnumpy().argmax(axis=1))))\n",
    "            print(\"[Train Correct     ] {}\".format(decode_char(tgt[0].asnumpy())))\n",
    "            print()\n",
    "            sw.flush()\n",
    "\n",
    "    test_loss = evaluate(net, test_iterator)\n",
    "    print(\"Epoch [{}], Train Loss {:.4f}, Test Loss {:.4f}\".format(e, loss/(i+1), test_loss))\n",
    "    sw.add_scalar(tag='Train_Loss', value={key:loss/(i+1)}, global_step=e)\n",
    "    sw.add_scalar(tag='Test_Loss', value={key:test_loss}, global_step=e)\n",
    "    print()\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(net, sentence):\n",
    "    scorer = nlp.model.BeamSearchScorer(alpha=0, K=2, from_logits=False)\n",
    "    beam_sampler = nlp.model.BeamSearchSampler(beam_size=5,\n",
    "                                           decoder=net.decode_logprob,\n",
    "                                           eos_id=EOS,\n",
    "                                           scorer=scorer,\n",
    "                                           max_length=150)\n",
    "    src_seq, src_valid_length = encode_char(sentence)\n",
    "    src_seq = mx.nd.array([src_seq], ctx=ctx)\n",
    "    src_valid_length = mx.nd.array(src_valid_length, ctx=ctx)\n",
    "    encoder_outputs, _ = net.encode(src_seq, valid_length=src_valid_length)\n",
    "    states = net.decoder.init_state_from_encoder(encoder_outputs, \n",
    "                                                      encoder_valid_length=src_valid_length)\n",
    "    inputs = mx.nd.full(shape=(1,), ctx=src_seq.context, dtype=np.float32, val=BOS)\n",
    "    samples, scores, valid_lengths = beam_sampler(inputs, states)\n",
    "    samples = samples[0].asnumpy()\n",
    "    scores = scores[0].asnumpy()\n",
    "    valid_lengths = valid_lengths[0].asnumpy()\n",
    "    return decode_char(samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 16\n",
    "embed_size = 256\n",
    "num_layers = 2\n",
    "\n",
    "epochs = 5\n",
    "key = 'language_denoising'\n",
    "best_test_loss = 10e20\n",
    "\n",
    "learning_rate = 0.00004\n",
    "send_every_n = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_loss = 10e20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/text_denoising'\n",
    "checkpoint_dir = \"model_checkpoint\"\n",
    "checkpoint_name = key+\".params\"\n",
    "sw = mxboard.SummaryWriter(logdir=log_dir, flush_secs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Denoiser(alphabet_size=len(ALPHABET), max_src_length=FEATURE_LEN, max_tgt_length=FEATURE_LEN, num_heads=num_heads, embed_size=embed_size, num_layers=num_layers)\n",
    "net.initialize(mx.init.Xavier(), ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(ALPHABET)\n",
    "label_smoothing = LabelSmoothing(epsilon=0.002, units=output_dim)\n",
    "loss_function_test = SoftmaxCEMaskedLoss(sparse_label=True)\n",
    "loss_function = SoftmaxCEMaskedLoss(sparse_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters\n",
      "[Test Typo     ] his #prescriptio pad, and\n",
      "[Test Predicted] his prescription paid, and\n",
      "[Test Correct  ] his # prescription pad, and\n",
      "0.08434206157922745\n"
     ]
    }
   ],
   "source": [
    "if (os.path.isfile(os.path.join(checkpoint_dir, checkpoint_name))):\n",
    "    net.load_parameters(os.path.join(checkpoint_dir, checkpoint_name), ctx=ctx)    \n",
    "    print(\"Loaded parameters\")\n",
    "    best_test_loss = evaluate(net, val_data_ft)\n",
    "    print(best_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters\n",
      "[Test Typo     ] nothing to tell me yet. But well be meeting\n",
      "[Test Predicted] nothing to tell me yet.  But well be meeting\n",
      "[Test Correct  ] nothing to tell me, yet. But we'll be meeting\n",
      "0.09212780237197876\n"
     ]
    }
   ],
   "source": [
    "model_path = 'models/denoiser2.params'\n",
    "if (os.path.isfile(model_path)):\n",
    "    net.load_parameters(model_path, ctx=ctx)    \n",
    "    print(\"Loaded parameters\")\n",
    "    best_test_loss = evaluate(net, val_data_ft)\n",
    "    print(best_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 0.00001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the public novel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'language_denoising'\n",
    "for e in range(epochs):\n",
    "    test_loss = run_epoch(net, e, train_data, val_data_ft, trainer)\n",
    "    if test_loss < best_test_loss:\n",
    "        print(\"Saving network, previous best test loss {:.6f}, current test loss {:.6f}\".format(best_test_loss, test_loss))\n",
    "        denoiser.save_parameters(os.path.join(checkpoint_dir, checkpoint_name))\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the GBW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'language_denoising_gbw'\n",
    "for e, corpus in enumerate(gbw_stream):\n",
    "    dataset_gbw = NoisyTextDataset(gbw_corpus=corpus, data_type='GBW').transform(transform)\n",
    "    train_data_gbw = gluon.data.DataLoader(dataset_gbw, batch_size=BATCH_SIZE, shuffle=True, last_batch='discard', batchify_fn=batchify, num_workers=5)\n",
    "    test_loss = run_epoch(net, e, train_data_gbw, val_data_ft, trainer)\n",
    "    if test_loss < best_test_loss:\n",
    "        print(\"Saving network, previous best test loss {:.6f}, current test loss {:.6f}\".format(best_test_loss, test_loss))\n",
    "        net.save_parameters(os.path.join(checkpoint_dir, checkpoint_name))\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning on the IAM training dataset text to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Typo     ] Company's Regulations.\" Of course not,\" agreed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] Company's Regulations. \"Of course not,\" agreed\n",
      "[Test Correct  ] Company's Regulations.\" \"Of course not,\" agreed\n",
      "[Iteration 0 Train] 0.03644362464547157\n",
      "[Iteration 0 Test ] 0.12242870777845383\n",
      "[Train Typo        ] featherweight cont1est betwoen Chris Elliot -- but\n",
      "[Train Predicted   ] featherwweight contest between Chris Elliot --but\n",
      "[Train Correct     ] feather-weight contest between Chris Elliot --but\n",
      "\n",
      "[Test Typo     ] world. You must help to lead our force.' The long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] world. You must help to lead our force. 'The long\n",
      "[Test Correct  ] World. You must help to lead our force.' The long\n",
      "[Iteration 50 Train] 0.046560638237233255\n",
      "[Iteration 50 Test ] 0.1202925142645836\n",
      "[Train Typo        ] oft tke exac oLition of F on hhc side\n",
      "[Train Predicted   ] off the exact polition of F on the side\n",
      "[Train Correct     ] off the exact position of F on the side\n",
      "\n",
      "[Test Typo     ] it dies when it changes into a tigerfly.' 'You are stil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] it is when it changes into a tigerfly. '' 'You are still\n",
      "[Test Correct  ] it dies when it changes into a tigerfly.' 'You are still\n",
      "[Iteration 100 Train] 0.04639535585399902\n",
      "[Iteration 100 Test ] 0.12085757941007615\n",
      "[Train Typo        ] my job is to build upthe U.S. apparatus which\n",
      "[Train Predicted   ] my job is to build up the U.S. apparatus which\n",
      "[Train Correct     ] my job is to build up the U.S. apparatus which\n",
      "\n",
      "[Test Typo     ] must present an unnffled appearance and carry onas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] must content an unfunfled appearance and carry on as\n",
      "[Test Correct  ] must present an unruffled appearance and carry on as\n",
      "Epoch [0], Train Loss 0.0463, Test Loss 0.1215\n",
      "\n",
      "[Test Typo     ] Camier knows what's going on. If he deesnit mind,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] Can Cort knows what's going on. If he doesnt mind,\n",
      "[Test Correct  ] Courier knows what's going on. If he doesn't mind,\n",
      "[Iteration 0 Train] 0.0453956238925457\n",
      "[Iteration 0 Test ] 0.12113679528236389\n",
      "[Train Typo        ] the hnborn cild days -- as Amuch an to the\n",
      "[Train Predicted   ] the unborn child days--as much as to the\n",
      "[Train Correct     ] the unborn child days--as much as to the\n",
      "\n",
      "[Test Typo     ] A light woind cafted the smoke of diesel exhaust in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] At A light wound crafted the smoke of diesel exhaust in\n",
      "[Test Correct  ] A light wind wafted the smoke of diesel exhaust in\n",
      "[Iteration 50 Train] 0.045796049941404196\n",
      "[Iteration 50 Test ] 0.1197916954755783\n",
      "[Train Typo        ] jointhe Cabiuet ( 4p]resmbly feeling tlal a moosteche miyht enhance\n",
      "[Train Predicted   ] join the Cabinet ( presumably feeling that a moostache might enhance\n",
      "[Train Correct     ] join the Cabinet ( presumably feeling that a moustache might enhance\n",
      "\n",
      "[Test Typo     ] have married a parson.\" Ite Rissed her. \"Parsons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] have married a parson. \"Its Rissed her.\" Parsons\n",
      "[Test Correct  ] have married a parson.\" He kissed her. \"Parsons\n",
      "[Iteration 100 Train] 0.04675204155616241\n",
      "[Iteration 100 Test ] 0.11972822308540344\n",
      "[Train Typo        ] swamped by inflated labour. Our industrKy, he sacd, would\n",
      "[Train Predicted   ] swamped by inflated labour. Our industry, he said, would\n",
      "[Train Correct     ] swamped by inflated labour. Our industry, he said, would\n",
      "\n",
      "[Test Typo     ] The conductor wwuitched on the lights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] The conductor wretched on the lights.\n",
      "[Test Correct  ] The conductor switched on the lights.\n",
      "Epoch [1], Train Loss 0.0467, Test Loss 0.1190\n",
      "\n",
      "[Test Typo     ] sally and of caurse Mrs Saptinus, for surely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] salt sand of course Mrs Saptinus, for surely\n",
      "[Test Correct  ] Sally and of course Mrs Septimus, for surely\n",
      "[Iteration 0 Train] 0.05174433812499046\n",
      "[Iteration 0 Test ] 0.11917565047740936\n",
      "[Train Typo        ] atlg tvue. Many people wake p gruny of a)marninVnd Scat\n",
      "[Train Predicted   ] aaltly true. Many people make ap grunp  of a marning and Scort\n",
      "[Train Correct     ] partly true. Many people wake up grumpy of a morning and Scant\n",
      "\n",
      "[Test Typo     ] Bawley?\" \"A pressman is always on the job\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] Bawley? \"\" A pressman is always on the job \"\n",
      "[Test Correct  ] Bawley?\" \"A pressman is always on the job.\"\n",
      "[Iteration 50 Train] 0.04474026782839906\n",
      "[Iteration 50 Test ] 0.11612725615501404\n",
      "[Train Typo        ] wrohe, 'is arrived, Who is a excitepneoure. Vosuvius\n",
      "[Train Predicted   ] wrote, 'is arrived, Who is a exciteng nesource. Vosuvius\n",
      "[Train Correct     ] wrote, 'is arrived, Who is a exciting resource. Vesuvius\n",
      "\n",
      "[Test Typo     ] foud Fueno Buck, now on th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] found Fueno Buck, now on the\n",
      "[Test Correct  ] found Bueno Buck, now on the\n",
      "[Iteration 100 Train] 0.04555345547966438\n",
      "[Iteration 100 Test ] 0.11408302396535873\n",
      "[Train Typo        ] The lat-er do nrt renponsubi@lities thems@elves as epert\n",
      "[Train Predicted   ] The lateer do not responsibilities themselves as expert\n",
      "[Train Correct     ] The latter do not responsibilities themselves as expert\n",
      "\n",
      "[Test Typo     ] mearosis and not insome eruption from those\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] means and not in some eruption from those\n",
      "[Test Correct  ] neurosis and not in some eruption from those\n",
      "Epoch [2], Train Loss 0.0456, Test Loss 0.1140\n",
      "\n",
      "[Test Typo     ] lrisses on the way back to the hakel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] tribes on the way back to the hotel.\n",
      "[Test Correct  ] kisses on the way back to the hotel.\n",
      "[Iteration 0 Train] 0.05650048330426216\n",
      "[Iteration 0 Test ] 0.1137312388420105\n",
      "[Train Typo        ] day-trip whil 6he whole tramsactionwas\n",
      "[Train Predicted   ] day-trip while the whole transaction was\n",
      "[Train Correct     ] day-trip while the whole transaction was\n",
      "\n",
      "[Test Typo     ] to help her control her feeling.. \" Come and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] to help her control her feeling.. \"Come and\n",
      "[Test Correct  ] to help her control her feelings. \"Come and\n",
      "[Iteration 50 Train] 0.04672745067407103\n",
      "[Iteration 50 Test ] 0.11305722117424011\n",
      "[Train Typo        ] oaalle, and anrsH af The p8aper was yivon up\n",
      "[Train Predicted   ] cablle, and a  aost of The paper was given up\n",
      "[Train Correct     ] Gaulle, and an rest of The paper was given up\n",
      "\n",
      "[Test Typo     ] should she tie? Beside, it wasu't fjist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] should she tie? Besides, it wasn't first\n",
      "[Test Correct  ] should she lie? Besides, it wasn't just\n",
      "[Iteration 100 Train] 0.047202259154603035\n",
      "[Iteration 100 Test ] 0.11213374257087708\n",
      "[Train Typo        ] I wt lhese, in eve aster orbut ,when hal\n",
      "[Train Predicted   ] I was there, in ever master or ut, when hhat\n",
      "[Train Correct     ] I was there, in even faster orbit, when that\n",
      "\n",
      "[Test Typo     ] ts get sut her compact and sow Grace's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] to get out her compact and sow Grace's\n",
      "[Test Correct  ] to get out her compact and saw Grace's\n",
      "Epoch [3], Train Loss 0.0470, Test Loss 0.1119\n",
      "\n",
      "[Test Typo     ] too. They're both here.\" \"Idich't know she was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] to. They're both here. \"\" I didn't know she was\n",
      "[Test Correct  ] too. They're both here.\" \"I didn't know she was\n",
      "[Iteration 0 Train] 0.04367374628782272\n",
      "[Iteration 0 Test ] 0.1116147756576538\n",
      "[Train Typo        ] Tho jokcs weve a shufla f tke predintanle od\n",
      "[Train Predicted   ] The jokes were a sufuuffla of the predictable odd\n",
      "[Train Correct     ] The jokes were a reshuffle of the predictable old\n",
      "\n",
      "[Test Typo     ] Around thast rauneled roely promuontory where the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] Around that reuncled rarely prominatory where the\n",
      "[Test Correct  ] Around that rounded rocky promontory where the\n",
      "[Iteration 50 Train] 0.04615194299349598\n",
      "[Iteration 50 Test ] 0.11349420785903931\n",
      "[Train Typo        ] Saying a largFe blondyouth ot ?quite\n",
      "[Train Predicted   ] Saying a large blond youth of quite\n",
      "[Train Correct     ] Saying a large blond youth of quite\n",
      "\n",
      "[Test Typo     ] Salits. \"Prisl agin and call each other wallahs,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n",
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] Salis. \"Prise again and call each other wallas,\n",
      "[Test Correct  ] habits. 'Drink gin and call each other wallahs,\n",
      "[Iteration 100 Train] 0.045559374412687696\n",
      "[Iteration 100 Test ] 0.11508634567260742\n",
      "[Train Typo        ] crowd anc drove back to Vence.\n",
      "[Train Predicted   ] crowd and drove back to Vence.\n",
      "[Train Correct     ] crowd and drove back to Vence.\n",
      "\n",
      "[Test Typo     ] minging of a doorbell was to him\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Predicted] mingling of a doorbell was to him\n",
      "[Test Correct  ] ringing of a doorbell was to him\n",
      "Epoch [4], Train Loss 0.0456, Test Loss 0.1161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mxboard.event_file_writer:wrote 1 event to disk\n"
     ]
    }
   ],
   "source": [
    "key = 'language_denoising_ft'\n",
    "for e in range(epochs):\n",
    "    test_loss = run_epoch(net, e, train_data_ft, val_data_ft, trainer)\n",
    "    if test_loss < best_test_loss:\n",
    "        print(\"Saving network, previous best test loss {:.6f}, current test loss {:.6f}\".format(best_test_loss, test_loss))\n",
    "        net.save_parameters(os.path.join(checkpoint_dir, checkpoint_name))\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This sentence contains an eror\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence(net, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix (maybe useful later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create text file with all vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = nlp.model.big_rnn_lm_2048_512(dataset_name='gbw', pretrained=True, ctx=mx.cpu())\n",
    "vocab_ = '\\n'.join(vocab.idx_to_token)\n",
    "with open('dataset/typo/vocab.txt', 'w') as f:\n",
    "    f.write(vocab_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create KNN lookup for words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
