{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Denoising\n",
    "\n",
    "Inspired by \"Neural Networks for Text Correction and Completion in Keyboard Decoding\" by Shaona Ghosh and Per Ola Kristensson. https://arxiv.org/pdf/1709.06429.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Append, Pad\n",
    "import gluonnlp as nlp\n",
    "import mxboard\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocr.utils.encoder_decoder import get_transformer_encoder_decoder, Denoiser, encode_char, decode_char, LabelSmoothing, SoftmaxCEMaskedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "if not os.path.isdir('dataset/typo'):\n",
    "    os.makedirs('dataset/typo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/typo/all.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filepath = 'dataset/typo/all.txt'\n",
    "mx.test_utils.download('https://s3.us-east-2.amazonaws.com/gluon-ocr/models/all.txt', dirname='dataset/typo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = ['<UNK>', '<PAD>', '<BOS>', '<EOS>']+list(' ' + string.ascii_letters + string.digits + string.punctuation)\n",
    "ALPHABET_INDEX = {letter: index for index, letter in enumerate(ALPHABET)} # { a: 0, b: 1, etc}\n",
    "FEATURE_LEN = 150 # max-length in characters for one document\n",
    "NUM_WORKERS = 8 # number of workers used in the data loading\n",
    "BATCH_SIZE = 64 # number of documents per batch\n",
    "MAX_LEN_SENTENCE = 150\n",
    "PAD = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "UNK = 0\n",
    "\n",
    "moses_detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "moses_tokenizer = nlp.data.SacreMosesTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTextDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 text_filepath=None, \n",
    "                 substitute_costs_filepath='models/substitute_probs.json', \n",
    "                 insert_weight=1, \n",
    "                 delete_weight=1, \n",
    "                 glue_prob=0.05, \n",
    "                 substitute_weight=2,\n",
    "                 max_replace=0.3,\n",
    "                 is_train=True, \n",
    "                 split=0.9, \n",
    "                 data_type='corpus', \n",
    "                 gbw_corpus=None\n",
    "                ):\n",
    "        self.max_replace = max_replace\n",
    "        self.replace_weight = 0 #replace_prob  #Ignore typo dataset\n",
    "        self.substitute_threshold = float(substitute_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.insert_threshold = self.substitute_threshold + float(insert_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.delete_threshold = self.insert_threshold + float(delete_weight) / (insert_weight + delete_weight + substitute_weight)\n",
    "        self.glue_prob = glue_prob\n",
    "        self.substitute_dict = json.load(open(substitute_costs_filepath,'r'))\n",
    "        self.split = split\n",
    "        self.data_type = data_type\n",
    "        if self.data_type == 'corpus':\n",
    "            self.text = self._process_text(text_filepath, is_train)\n",
    "        elif self.data_type == 'GBW':\n",
    "            self.gbw_corpus = gbw_corpus\n",
    "        \n",
    "    def _process_text(self, filename, is_train):\n",
    "        with open(filename, 'r', encoding='Latin-1') as f:\n",
    "            text = []\n",
    "            for line in f.readlines():\n",
    "                if line != '':\n",
    "                    text.append(line.strip())\n",
    "            \n",
    "            split_index = int(self.split*len(text))\n",
    "            if is_train:\n",
    "                text = text[:split_index]\n",
    "            else:\n",
    "                text = text[split_index:]\n",
    "        return text\n",
    "    \n",
    "    def _transform_line(self, line):\n",
    "        \"\"\"\n",
    "        replace words that are in the typo dataset with a typo\n",
    "        with a probability `self.replace_proba`\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        \n",
    "        processed_line = self._pre_process_line(line)\n",
    "        \n",
    "        # We get randomly the index of the modifications\n",
    "        num_chars = len(''.join(processed_line))\n",
    "        if num_chars:\n",
    "            index_modifications = np.random.choice(num_chars, random.randint(0, int(self.max_replace*num_chars)), replace=False)\n",
    "            substitute_letters = []\n",
    "            insert_letters = []\n",
    "            delete_letters = []\n",
    "            # We randomly assign these indices to modifications based on precalculated thresholds\n",
    "            for index in index_modifications:\n",
    "                draw = random.random()\n",
    "                if draw < self.substitute_threshold:\n",
    "                    substitute_letters.append(index)\n",
    "                    continue\n",
    "                if draw < self.insert_threshold:\n",
    "                    insert_letters.append(index)\n",
    "                    continue\n",
    "                else:\n",
    "                    delete_letters.append(index)\n",
    "                            \n",
    "        \n",
    "        j = 0\n",
    "        for i, word in enumerate(processed_line):\n",
    "            if word != '' and word not in string.punctuation:\n",
    "                len_word = len(word)\n",
    "                word_ = []\n",
    "                k = j\n",
    "                for letter in word:\n",
    "                    if k in substitute_letters and letter in self.substitute_dict:\n",
    "                        draw = random.random()\n",
    "                        for replace, prob in self.substitute_dict[letter].items():\n",
    "                            if draw < prob:\n",
    "                                letter = replace\n",
    "                                break\n",
    "                    word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                                \n",
    "                # Insert random letter\n",
    "                k = j\n",
    "                word_ = []\n",
    "                for letter in word:\n",
    "                    if k in insert_letters:\n",
    "                        word_.append(ALPHABET[random.randint(4, len(ALPHABET)-1)])\n",
    "                    word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                \n",
    "                # Delete random letter\n",
    "                k = j\n",
    "                word_ = []\n",
    "                for letter in word:\n",
    "                    if k not in delete_letters:\n",
    "                        word_.append(letter)\n",
    "                    k += 1\n",
    "                word = ''.join(word_)\n",
    "                    \n",
    "                output.append(word)\n",
    "            else:\n",
    "                output.append(word)\n",
    "            j += len(word)\n",
    "\n",
    "        output_ = [\"\"]*len(output)\n",
    "        j = 0\n",
    "        for i, word in enumerate(output):\n",
    "            output_[j] += word\n",
    "            if random.random() > self.glue_prob:\n",
    "                j += 1\n",
    "        \n",
    "        line = self._post_process_line(output_)\n",
    "        return line.strip()\n",
    "    \n",
    "    def _pre_process_line(self, line):\n",
    "        line = line.replace('\\n','').replace('`',\"'\").replace('--',' -- ')\n",
    "        return moses_tokenizer(line)\n",
    "        \n",
    "    def _post_process_line(self, words):\n",
    "        output = ' '.join(moses_detokenizer(words))\n",
    "        return output\n",
    "    \n",
    "    def _match_caps(self, original, typo):\n",
    "        if original.isupper():\n",
    "            return typo.upper()\n",
    "        elif original.istitle():\n",
    "            return typo.capitalize()\n",
    "        else:\n",
    "            return typo\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_type == 'GBW':\n",
    "            tokens = moses_detokenizer(self.gbw_corpus[idx][:-1])\n",
    "            if len(tokens) > 6:\n",
    "                start = random.randint(0, len(tokens)-3)\n",
    "                end = random.randint(start, len(tokens))\n",
    "                tokens = tokens[start:end]\n",
    "            line = ' '.join(tokens)\n",
    "        else:\n",
    "            line = self.text[idx]\n",
    "        line_typo = self._transform_line(line)\n",
    "        return line_typo, line\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data_type == 'GBW':\n",
    "            return len(self.gbw_corpus)\n",
    "        else:\n",
    "            return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(text, src=True):\n",
    "    encoded = np.ones(FEATURE_LEN, dtype='float32') * PAD\n",
    "    text = text[:FEATURE_LEN-2]\n",
    "    i = 0\n",
    "    if not src:\n",
    "        encoded[0] = BOS\n",
    "        i = 1\n",
    "    for letter in text:\n",
    "        if letter in ALPHABET_INDEX:\n",
    "            encoded[i] = ALPHABET_INDEX[letter]\n",
    "        i += 1\n",
    "    encoded[i] = EOS\n",
    "    return encoded, np.array([i+1]).astype('float32')\n",
    "\n",
    "def encode_word(text, src=True):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = vocab[tokens]\n",
    "    indices += [vocab['<EOS>']]\n",
    "    indices = [vocab['<BOS>']]+indices\n",
    "    return indices, np.array([len(indices)]).astype('float32')\n",
    "\n",
    "def transform(data, label):\n",
    "    src, src_valid_length = encode_char(data, src=True)\n",
    "    tgt, tgt_valid_length = encode_char(label, src=False)\n",
    "    return src, src_valid_length, tgt, tgt_valid_length, data, label\n",
    "\n",
    "def decode_char(text):\n",
    "    output = []\n",
    "    for val in text:\n",
    "        if val == EOS:\n",
    "            break\n",
    "        elif val == PAD or val == BOS:\n",
    "            continue\n",
    "        output.append(ALPHABET[int(val)])\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "def decode_word(indices):\n",
    "    return detokenizer([vocab.idx_to_token[int(i)] for i in indices], return_str=True).replace('<PAD>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NoisyTextDataset(text_filepath=text_filepath, glue_prob=0.2, is_train=True).transform(transform)\n",
    "dataset_test = NoisyTextDataset(text_filepath=text_filepath, glue_prob=0.2, is_train=False).transform(transform)\n",
    "\n",
    "# Finetuning on the text from the IAM dataset\n",
    "dataset_train_ft = NoisyTextDataset(text_filepath='dataset/typo/text_train.txt', is_train=True, split=1.0).transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([57., 58., 59., 82., 57., 57., 61.,  4., 31., 18.,  8.,  4., 24.,\n",
       "        12.,  9.,  4., 42., 45., 48., 34.,  4., 23., 20.,  5., 15.,  9.,\n",
       "         4., 23., 25.,  8.,  8.,  9., 18., 16., 11.,  4., 25.,  7., 24.,\n",
       "        19.,  4., 63., 43., 19., 23.,  9., 23., 78.,  4.,  5., 18.,  8.,\n",
       "         4., 25., 18., 24., 19.,  4., 31.,  5., 22., 19., 18., 78.,  4.,\n",
       "         5., 18.,  8.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([69.], dtype=float32),\n",
       " array([ 2., 57., 58., 59., 82., 57., 57., 61.,  4., 31., 18.,  8.,  4.,\n",
       "        24., 12.,  9.,  4., 42., 45., 48., 34.,  4., 23., 20.,  5., 15.,\n",
       "         9.,  4., 23., 25.,  8.,  8.,  9., 18., 16., 29.,  4., 25., 18.,\n",
       "        24., 19.,  4., 43., 19., 23.,  9., 23., 78.,  4.,  5., 18.,  8.,\n",
       "         4., 25., 18., 24., 19.,  4., 31.,  5., 22., 19., 18., 78.,  4.,\n",
       "         5., 18.,  8.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([69.], dtype=float32),\n",
       " '012:004 And the LORD spake suddenlg ucto 6Moses, and unto Aaron, and',\n",
       " '012:004 And the LORD spake suddenly unto Moses, and unto Aaron, and')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[random.randint(0, len(dataset_train)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data being the IAM Dataset prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('dataset/typo/validating.json','r'))\n",
    "data_ = []\n",
    "for label, modified in data:\n",
    "    if label.strip() != modified.strip():\n",
    "        data_.append([label, modified])\n",
    "val_dataset_ft = gluon.data.ArrayDataset(list(list(zip(*data_))[1]), list(list(zip(*data_))[0])).transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17., 19., 22.,  9.,  4., 24., 12.,  5., 24.,  4., 24., 27.,  9.,\n",
       "        18., 24., 29., 79., 16., 19., 25., 22.,  4., 12., 19., 25., 23.,\n",
       "         4.,  9.,  5., 22., 16., 13.,  9., 22., 80.,  4., 49., 13., 18.,\n",
       "         7.,  9., 80.,  4., 24., 12.,  9., 18.,  3.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([48.], dtype=float32),\n",
       " array([ 2., 17., 19., 22.,  9.,  4., 24., 12.,  5., 18.,  4., 24., 27.,\n",
       "         9., 18., 24., 29., 79., 10., 19., 25., 22.,  4., 12., 19., 25.,\n",
       "        22., 23.,  4.,  9.,  5., 22., 16., 13.,  9., 22., 80.,  4., 49.,\n",
       "        13., 18.,  7.,  9.,  4., 24., 12.,  9., 18.,  3.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([49.], dtype=float32),\n",
       " 'more that twenty-lour hous earlier. Since. then',\n",
       " 'more than twenty-four hours earlier. Since then')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_ft[random.randint(0, len(val_dataset_ft)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GBW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbw_stream = nlp.data.GBWStream(segment='train', skip_empty=True, bos=None, eos='<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, corpus in enumerate(gbw_stream):\n",
    "    dataset_gbw = NoisyTextDataset(gbw_corpus=corpus, data_type='GBW').transform(transform)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20., 22., 19., 23., 20.,  9.,  7., 24., 23.,  4., 24., 12., 22.,\n",
       "        19., 25., 11., 12.,  4., 59., 57., 58., 57., 78.,  4., 27., 13.,\n",
       "        24., 12.,  4.,  5.,  4., 23., 13., 11., 18., 13., 10., 13.,  7.,\n",
       "         5., 18., 24.,  4.,  9.,  7., 19., 18., 19., 17., 13.,  7.,  3.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([52.], dtype=float32),\n",
       " array([ 2., 20., 22., 19., 23., 20.,  9.,  7., 24., 23.,  4., 24., 12.,\n",
       "        22., 19., 25., 11., 12.,  4., 59., 57., 58., 57., 78.,  4., 27.,\n",
       "        13., 24., 12.,  4.,  5.,  4., 23., 13., 11., 18., 13., 10., 13.,\n",
       "         7.,  5., 18., 24.,  4.,  9.,  7., 19., 18., 19., 17., 13.,  7.,\n",
       "         3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32),\n",
       " array([53.], dtype=float32),\n",
       " 'prospects through 2010, with a significant economic',\n",
       " 'prospects through 2010, with a significant economic')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gbw[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_list(elem):\n",
    "    output = []\n",
    "    for e in elem:\n",
    "        output.append(elem)\n",
    "    return output\n",
    "    \n",
    "batchify = Tuple(Stack(), Stack(), Stack(), Stack(), batchify_list, batchify_list)\n",
    "batchify_word = Tuple(Stack(), Stack(), Pad(), Stack(), batchify_list, batchify_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, last_batch='rollover', batchify_fn=batchify, num_workers=5)\n",
    "test_data = gluon.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True, last_batch='keep', batchify_fn=batchify, num_workers=5)\n",
    "val_data_ft = gluon.data.DataLoader(val_dataset_ft, batch_size=BATCH_SIZE, shuffle=True, last_batch='keep', batchify_fn=batchify, num_workers=0)\n",
    "train_data_ft = gluon.data.DataLoader(dataset_train_ft, batch_size=BATCH_SIZE, shuffle=True, last_batch='rollover', batchify_fn=batchify, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to help train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, iterator):\n",
    "    loss = 0\n",
    "    for i, (src, src_valid_length, tgt, tgt_valid_length, typo, label) in enumerate(iterator):\n",
    "        src = src.as_in_context(ctx)\n",
    "        tgt = tgt.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx).squeeze()\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx).squeeze()\n",
    "        output = net(src, tgt[:,:-1], src_valid_length, tgt_valid_length-1)\n",
    "        ls = loss_function_test(output, tgt[:,1:], tgt_valid_length).mean()\n",
    "        loss += ls.asscalar()\n",
    "    print(\"[Test Typo     ] {}\".format(decode_char(src[0].asnumpy())))\n",
    "    print(\"[Test Predicted] {}\".format(get_sentence(net, decode_char(src[0].asnumpy()))))\n",
    "    print(\"[Test Correct  ] {}\".format(decode_char(tgt[0].asnumpy())))\n",
    "    return loss / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(net, epoch, train_iterator, test_iterator):\n",
    "    loss = 0.\n",
    "    for i, (src, src_valid_length, tgt, tgt_valid_length, typo, label) in enumerate(train_iterator):\n",
    "        src = src.as_in_context(ctx)\n",
    "        tgt = tgt.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx).squeeze()\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx).squeeze()\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = net(src, tgt[:,:-1], src_valid_length, tgt_valid_length-1)\n",
    "            smoothed_label = label_smoothing(tgt[:,1:])\n",
    "            ls = loss_function(output, smoothed_label, tgt_valid_length).mean()\n",
    "        \n",
    "        ls.backward()\n",
    "        trainer.step(src.shape[0])\n",
    "        loss += ls.asscalar()\n",
    "        \n",
    "        if i % send_every_n == 0:\n",
    "            val_loss = evaluate(net, test_iterator)\n",
    "            sw.add_scalar(tag='Val_Loss_it', value={key:val_loss}, global_step=i+e*len(train_iterator))\n",
    "            sw.add_scalar(tag='Train_Loss_it', value={key:loss/(i+1)}, global_step=i+e*len(train_iterator))\n",
    "            print(\"[Iteration {} Train] {}\".format(i, loss / (i+1)))\n",
    "            print(\"[Iteration {} Test ] {}\".format(i, val_loss))\n",
    "            print(\"[Train Typo        ] {}\".format(decode_char(src[0].asnumpy())))\n",
    "            print(\"[Train Predicted   ] {}\".format(decode_char(output[0].asnumpy().argmax(axis=1))))\n",
    "            print(\"[Train Correct     ] {}\".format(decode_char(tgt[0].asnumpy())))\n",
    "            print()\n",
    "            sw.flush()\n",
    "\n",
    "    test_loss = evaluate(denoiser, test_iterator)\n",
    "    print(\"Epoch [{}], Train Loss {:.4f}, Test Loss {:.4f}\".format(e, loss/(i+1), test_loss))\n",
    "    sw.add_scalar(tag='Train_Loss', value={key:loss/(i+1)}, global_step=e)\n",
    "    sw.add_scalar(tag='Test_Loss', value={key:test_loss}, global_step=e)\n",
    "    print()\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 16\n",
    "embed_size = 256\n",
    "num_layers = 2\n",
    "\n",
    "epochs = 1\n",
    "key = 'language_denoising'\n",
    "best_test_loss = 10e20\n",
    "\n",
    "learning_rate = 0.00004\n",
    "send_every_n = 50\n",
    "best_test_loss = 10e20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/text_denoising'\n",
    "checkpoint_dir = \"model_checkpoint\"\n",
    "checkpoint_name = key+\".params\"\n",
    "sw = mxboard.SummaryWriter(logdir=log_dir, flush_secs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Denoiser(alphabet_size=len(ALPHABET), max_src_length=FEATURE_LEN, max_tgt_length=FEATURE_LEN, num_heads=num_heads, embed_size=embed_size, num_layers=num_layers)\n",
    "net.initialize(mx.init.Xavier(), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile(os.path.join(checkpoint_dir, checkpoint_name))):\n",
    "    net.load_parameters(os.path.join(checkpoint_dir, checkpoint_name), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = LabelSmoothing(epsilon=0.002, units=output_dim)\n",
    "loss_function_test = SoftmaxCEMaskedLoss(sparse_label=True)\n",
    "loss_function = SoftmaxCEMaskedLoss(sparse_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate':learning_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the public novel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    test_loss = run_epoch(net, e, train_data, val_data_ft)\n",
    "    if test_loss < best_test_loss:\n",
    "        print(\"Saving network, previous best test loss {:.6f}, current test loss {:.6f}\".format(best_test_loss, test_loss))\n",
    "        denoiser.save_parameters(os.path.join(checkpoint_dir, checkpoint_name))\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the GBW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, corpus in enumerate(gbw_stream):\n",
    "    dataset_gbw = NoisyTextDataset(gbw_corpus=corpus, data_type='GBW').transform(transform)\n",
    "    train_data_gbw = gluon.data.DataLoader(dataset_gbw, batch_size=BATCH_SIZE, shuffle=True, last_batch='discard', batchify_fn=batchify, num_workers=5)\n",
    "    test_loss = train_epoch(net, e, train_data_gbw, val_data_ft)\n",
    "    if test_loss < best_test_loss:\n",
    "        print(\"Saving network, previous best test loss {:.6f}, current test loss {:.6f}\".format(best_test_loss, test_loss))\n",
    "        denoiser.save_parameters(os.path.join(checkpoint_dir, checkpoint_name))\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning on the IAM dataset text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    test_loss = train_epoch(net, e, train_data_ft, val_data_ft)\n",
    "    if test_loss < best_test_loss:\n",
    "        print(\"Saving network, previous best test loss {:.6f}, current test loss {:.6f}\".format(best_test_loss, test_loss))\n",
    "        denoiser.save_parameters(os.path.join(checkpoint_dir, checkpoint_name))\n",
    "        best_test_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(net, sentence):\n",
    "    scorer = nlp.model.BeamSearchScorer(alpha=0, K=2, from_logits=False)\n",
    "    beam_sampler = nlp.model.BeamSearchSampler(beam_size=5,\n",
    "                                           decoder=net.decode_logprob,\n",
    "                                           eos_id=EOS,\n",
    "                                           scorer=scorer,\n",
    "                                           max_length=150)\n",
    "    src_seq, src_valid_length = encode_char(sentence)\n",
    "    src_seq = mx.nd.array([src_seq], ctx=ctx)\n",
    "    src_valid_length = mx.nd.array(src_valid_length, ctx=ctx)\n",
    "    encoder_outputs, _ = net.encode(src_seq, valid_length=src_valid_length)\n",
    "    states = net.decoder.init_state_from_encoder(encoder_outputs, \n",
    "                                                      encoder_valid_length=src_valid_length)\n",
    "    inputs = mx.nd.full(shape=(1,), ctx=src_seq.context, dtype=np.float32, val=BOS)\n",
    "    samples, scores, valid_lengths = beam_sampler(inputs, states)\n",
    "    samples = samples[0].asnumpy()\n",
    "    scores = scores[0].asnumpy()\n",
    "    valid_lengths = valid_lengths[0].asnumpy()\n",
    "    return decode_char(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This sentence contains an eror\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence(net, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix (maybe useful later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text file with all vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = nlp.model.big_rnn_lm_2048_512(dataset_name='gbw', pretrained=True, ctx=mx.cpu())\n",
    "vocab = '\\n'.join(vocab.idx_to_token)\n",
    "with open('dataset/typo/vocab.txt', 'w') as f:\n",
    "    f.write(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
